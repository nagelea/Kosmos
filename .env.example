# Kosmos AI Scientist Configuration
# Copy this file to .env and fill in your values

# ============================================================================
# CLAUDE CONFIGURATION
# ============================================================================

# Option 1: Use Claude Code CLI (Recommended for Max subscription users)
# Set API key to all 9s to route to local Claude Code CLI
# Requires: pip install git+https://github.com/jimmc414/claude_n_codex_api_proxy.git
# Requires: claude CLI installed and authenticated
ANTHROPIC_API_KEY=999999999999999999999999999999999999999999999999

# Option 2: Use Anthropic API (Pay-per-use)
# Get your API key from https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-api03-your-actual-key-here

# Claude model to use (only for API mode, CLI uses Max subscription default)
# Options: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-haiku-20240307
CLAUDE_MODEL=claude-3-5-sonnet-20241022

# Maximum tokens per request
CLAUDE_MAX_TOKENS=4096

# Temperature for generation (0.0-1.0, higher = more creative)
CLAUDE_TEMPERATURE=0.7

# Enable prompt caching to reduce API costs (true/false)
CLAUDE_ENABLE_CACHE=true

# ============================================================================
# RESEARCH CONFIGURATION
# ============================================================================

# Maximum research iterations before convergence check
MAX_RESEARCH_ITERATIONS=10

# Research budget in USD for API costs (0.0 = unlimited)
RESEARCH_BUDGET_USD=10.0

# Domains to enable
# Format: Comma-separated values OR JSON array
# Options: biology, physics, chemistry, neuroscience, materials, social_science
# Examples:
#   ENABLED_DOMAINS=biology,physics,chemistry,neuroscience
#   ENABLED_DOMAINS=["biology","physics","chemistry","neuroscience"]
ENABLED_DOMAINS=biology,physics,chemistry,neuroscience

# Experiment types to allow
# Format: Comma-separated values OR JSON array
# Options: computational, data_analysis, literature_synthesis, simulation
# Examples:
#   ENABLED_EXPERIMENT_TYPES=computational,data_analysis,literature_synthesis
#   ENABLED_EXPERIMENT_TYPES=["computational","data_analysis","literature_synthesis"]
ENABLED_EXPERIMENT_TYPES=computational,data_analysis,literature_synthesis

# Minimum novelty score for hypothesis (0.0-1.0)
MIN_NOVELTY_SCORE=0.6

# Enable autonomous iteration (true/false)
ENABLE_AUTONOMOUS_ITERATION=true

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

# Database URL for SQLAlchemy
# SQLite (default, single file): sqlite:///kosmos.db
# PostgreSQL (production, with Docker): postgresql://kosmos:kosmos-dev-password@localhost:5432/kosmos
# PostgreSQL (custom): postgresql://user:password@host:port/database
DATABASE_URL=sqlite:///kosmos.db

# Enable database echo (SQL logging for debugging)
DATABASE_ECHO=false

# ============================================================================
# REDIS CACHE (Optional)
# ============================================================================

# Enable Redis caching (requires Redis server or Docker)
# Set to true to use Redis for caching (faster than in-memory cache)
REDIS_ENABLED=false

# Redis connection URL
# Format: redis://host:port/db
# Default: redis://localhost:6379/0
# With Docker: redis://localhost:6379/0 (docker-compose handles this)
REDIS_URL=redis://localhost:6379/0

# Maximum Redis connection pool size (1-1000, default: 50)
REDIS_MAX_CONNECTIONS=50

# Socket timeout in seconds (1-60, default: 5)
REDIS_SOCKET_TIMEOUT=5

# Socket connect timeout in seconds (1-60, default: 5)
REDIS_SOCKET_CONNECT_TIMEOUT=5

# Retry on timeout (true/false, default: true)
REDIS_RETRY_ON_TIMEOUT=true

# Decode responses as UTF-8 strings (true/false, default: true)
REDIS_DECODE_RESPONSES=true

# Default cache TTL in seconds (60-86400, default: 3600 = 1 hour)
REDIS_DEFAULT_TTL_SECONDS=3600

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format: json or text
LOG_FORMAT=json

# Log file path (leave empty to log to stdout only)
LOG_FILE=logs/kosmos.log

# Enable debug mode (verbose output)
DEBUG_MODE=false

# ============================================================================
# LITERATURE APIS (Optional)
# ============================================================================

# Semantic Scholar API key (optional, increases rate limits)
# Get from: https://www.semanticscholar.org/product/api
# SEMANTIC_SCHOLAR_API_KEY=your-key-here

# PubMed API key (optional, increases rate limits)
# Get from: https://www.ncbi.nlm.nih.gov/account/
# PUBMED_API_KEY=your-key-here

# Email for PubMed E-utilities (recommended, helps with rate limits)
# PUBMED_EMAIL=your-email@example.com

# Literature cache TTL in hours (24-168, default: 48)
LITERATURE_CACHE_TTL_HOURS=48

# Maximum results per literature search query (1-1000, default: 100)
MAX_RESULTS_PER_QUERY=100

# PDF download timeout in seconds (5-120, default: 30)
PDF_DOWNLOAD_TIMEOUT=30

# ============================================================================
# VECTOR DATABASE (For semantic search)
# ============================================================================

# Vector DB type: chromadb, pinecone, weaviate
VECTOR_DB_TYPE=chromadb

# ChromaDB settings (local, no key needed)
CHROMA_PERSIST_DIRECTORY=.chroma_db

# Pinecone settings (if using Pinecone)
# PINECONE_API_KEY=your-key-here
# PINECONE_ENVIRONMENT=us-west1-gcp
# PINECONE_INDEX_NAME=kosmos

# ============================================================================
# NEO4J KNOWLEDGE GRAPH
# ============================================================================

# Neo4j connection URI
NEO4J_URI=bolt://localhost:7687

# Neo4j authentication
NEO4J_USER=neo4j
NEO4J_PASSWORD=kosmos-password

# Neo4j database name (default: neo4j)
NEO4J_DATABASE=neo4j

# Neo4j connection pool settings
NEO4J_MAX_CONNECTION_LIFETIME=3600
NEO4J_MAX_CONNECTION_POOL_SIZE=50

# ============================================================================
# DOMAIN-SPECIFIC APIS (Optional)
# ============================================================================

# Biology APIs
# KEGG_API_KEY=your-key-here
# UNIPROT_API_KEY=your-key-here

# Materials Science APIs
# MATERIALS_PROJECT_API_KEY=your-key-here

# Astronomy APIs
# NASA_API_KEY=your-key-here

# ============================================================================
# SAFETY CONFIGURATION
# ============================================================================

# Enable code safety checks
ENABLE_SAFETY_CHECKS=true

# Maximum execution time for experiments (seconds)
MAX_EXPERIMENT_EXECUTION_TIME=300

# Maximum memory usage (MB)
MAX_MEMORY_MB=2048

# Enable sandboxing for code execution
ENABLE_SANDBOXING=true

# Require human approval for high-risk operations
REQUIRE_HUMAN_APPROVAL=false

# ============================================================================
# PERFORMANCE CONFIGURATION
# ============================================================================

# Enable result caching
ENABLE_RESULT_CACHING=true

# Cache TTL (seconds)
CACHE_TTL=3600

# Number of parallel experiments (0 = sequential, default: 0)
# Recommended: CPU cores - 1 (e.g., 3 for 4-core, 7 for 8-core)
PARALLEL_EXPERIMENTS=0

# ============================================================================
# ASYNC LLM CONFIGURATION (For concurrent API calls)
# ============================================================================

# Maximum concurrent LLM API calls (1-20, default: 5)
# Higher values = faster but may hit rate limits
# Anthropic recommended: 5 for regular, 10 for rate limit increase
MAX_CONCURRENT_LLM_CALLS=5

# LLM rate limit per minute (1-100, default: 50)
# Set based on your Anthropic API tier limits
LLM_RATE_LIMIT_PER_MINUTE=50

# Maximum parallel hypothesis evaluations (1-10, default: 3)
MAX_PARALLEL_HYPOTHESIS_EVALUATIONS=3

# Enable concurrent result analysis (true/false, default: true)
ENABLE_CONCURRENT_RESULT_ANALYSIS=true

# ============================================================================
# PROFILING CONFIGURATION (For performance analysis)
# ============================================================================

# Enable performance profiling (true/false, default: false)
# Note: Profiling adds ~5-10% overhead in full mode
ENABLE_PROFILING=false

# Profiling mode: light, standard, full
# - light: Basic timing + memory (< 1% overhead)
# - standard: + cProfile for CPU profiling (~5% overhead)
# - full: + line-by-line profiling (~10-15% overhead)
PROFILING_MODE=light

# Store profile results in database (true/false, default: true)
STORE_PROFILE_RESULTS=true

# Profile data retention in days (1-90, default: 30)
PROFILE_STORAGE_DAYS=30

# Enable automatic bottleneck detection (true/false, default: true)
ENABLE_BOTTLENECK_DETECTION=true

# Bottleneck threshold percentage (1-50, default: 10)
# Functions taking > X% of total time are flagged
BOTTLENECK_THRESHOLD_PERCENT=10

# ============================================================================
# MONITORING & METRICS
# ============================================================================

# Enable usage statistics tracking
ENABLE_USAGE_STATS=true

# Metrics export interval (seconds, 0 = disabled)
METRICS_EXPORT_INTERVAL=60

# ============================================================================
# DEVELOPMENT SETTINGS
# ============================================================================

# Enable hot reload (development only)
HOT_RELOAD=false

# Enable API request logging
LOG_API_REQUESTS=false

# Test mode (uses mocks instead of real APIs)
TEST_MODE=false
