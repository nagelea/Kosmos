"""Report Generator for Kosmos E2E Testing

Generate formatted test reports and summaries.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Optional


def generate_report(
    results: dict,
    output_path: Optional[str] = None,
    format: str = "markdown",
) -> str:
    """Generate a test report

    Args:
        results: Test results dictionary from run_tests()
        output_path: Optional path to save report
        format: Output format ('markdown', 'json', 'text')

    Returns:
        Formatted report string
    """
    if format == "markdown":
        report = _generate_markdown(results)
    elif format == "json":
        report = _generate_json(results)
    else:
        report = _generate_text(results)

    if output_path:
        Path(output_path).write_text(report)
        print(f"Report saved to: {output_path}")

    return report


def _generate_markdown(results: dict) -> str:
    """Generate markdown format report"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    lines = [
        "# Kosmos E2E Test Report",
        "",
        f"**Generated:** {timestamp}",
        "",
        "## Summary",
        "",
        f"| Metric | Value |",
        f"|--------|-------|",
        f"| Tier | {results.get('tier', 'N/A')} |",
        f"| Provider | {results.get('provider', 'N/A')} |",
        f"| Passed | {results.get('passed', 0)} |",
        f"| Failed | {results.get('failed', 0)} |",
        f"| Skipped | {results.get('skipped', 0)} |",
        f"| Total | {results.get('total', 0)} |",
        f"| Duration | {results.get('duration', 0):.1f}s |",
        f"| Status | {'PASS' if results.get('success') else 'FAIL'} |",
        "",
    ]

    if results.get("coverage"):
        lines.extend([
            "## Coverage",
            "",
            f"**Total Coverage:** {results['coverage']}%",
            "",
        ])

    pass_rate = (results.get("passed", 0) / max(results.get("total", 1), 1)) * 100
    lines.extend([
        "## Analysis",
        "",
        f"- Pass rate: {pass_rate:.1f}%",
    ])

    if results.get("failed", 0) > 0:
        lines.append(f"- {results['failed']} test(s) failed - review needed")

    if results.get("skipped", 0) > 0:
        lines.append(f"- {results['skipped']} test(s) skipped")

    lines.extend([
        "",
        "---",
        "*Generated by Kosmos E2E Testing Skill*",
    ])

    return "\n".join(lines)


def _generate_json(results: dict) -> str:
    """Generate JSON format report"""
    report_data = {
        "timestamp": datetime.now().isoformat(),
        "results": results,
        "analysis": {
            "pass_rate": (results.get("passed", 0) / max(results.get("total", 1), 1)) * 100,
            "has_failures": results.get("failed", 0) > 0,
        },
    }
    return json.dumps(report_data, indent=2)


def _generate_text(results: dict) -> str:
    """Generate plain text report"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    lines = [
        "KOSMOS E2E TEST REPORT",
        "=" * 50,
        f"Generated: {timestamp}",
        "",
        "SUMMARY",
        "-" * 30,
        f"Tier:     {results.get('tier', 'N/A')}",
        f"Provider: {results.get('provider', 'N/A')}",
        f"Passed:   {results.get('passed', 0)}",
        f"Failed:   {results.get('failed', 0)}",
        f"Skipped:  {results.get('skipped', 0)}",
        f"Total:    {results.get('total', 0)}",
        f"Duration: {results.get('duration', 0):.1f}s",
        f"Status:   {'PASS' if results.get('success') else 'FAIL'}",
        "",
    ]

    if results.get("coverage"):
        lines.extend([
            "COVERAGE",
            "-" * 30,
            f"Total: {results['coverage']}%",
            "",
        ])

    return "\n".join(lines)


def print_summary(results: dict) -> None:
    """Print a concise test summary to console

    Args:
        results: Test results dictionary
    """
    status = "[PASS]" if results.get("success") else "[FAIL]"
    passed = results.get("passed", 0)
    failed = results.get("failed", 0)
    total = results.get("total", 0)
    duration = results.get("duration", 0)

    print()
    print("=" * 50)
    print(f"  {status} {results.get('tier', 'Test')} Results")
    print("=" * 50)
    print(f"  Passed:   {passed}/{total}")
    print(f"  Failed:   {failed}")
    print(f"  Duration: {duration:.1f}s")
    print(f"  Provider: {results.get('provider', 'unknown')}")
    print("=" * 50)
    print()


def compare_results(results_list: list[dict]) -> str:
    """Compare multiple test runs

    Args:
        results_list: List of results dictionaries

    Returns:
        Comparison report string
    """
    lines = [
        "# Test Comparison Report",
        "",
        "| Provider | Tier | Passed | Failed | Duration | Status |",
        "|----------|------|--------|--------|----------|--------|",
    ]

    for r in results_list:
        status = "PASS" if r.get("success") else "FAIL"
        line = (
            f"| {r.get('provider', 'N/A')} "
            f"| {r.get('tier', 'N/A')} "
            f"| {r.get('passed', 0)} "
            f"| {r.get('failed', 0)} "
            f"| {r.get('duration', 0):.1f}s "
            f"| {status} |"
        )
        lines.append(line)

    return "\n".join(lines)


def generate_dependency_report(
    project_root: str = ".",
    output_path: Optional[str] = None
) -> str:
    """Generate a comprehensive dependency analysis report

    Scans for missing dependencies and generates E2E_TESTING_DEPENDENCY_REPORT.md

    Args:
        project_root: Path to project root
        output_path: Optional output path (default: E2E_TESTING_DEPENDENCY_REPORT.md)

    Returns:
        Report content as string
    """
    from pathlib import Path
    import os
    import sys

    # Import provider_detector for infrastructure checks
    try:
        from .provider_detector import (
            detect_all, check_python_packages, check_python_version,
            get_service_matrix
        )
    except ImportError:
        from provider_detector import (
            detect_all, check_python_packages, check_python_version,
            get_service_matrix
        )

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    detection = detect_all()
    packages = check_python_packages()
    py_version = check_python_version()
    service_matrix = get_service_matrix()

    lines = [
        "# E2E Testing Dependency Report",
        "",
        f"**Generated:** {timestamp}",
        f"**Python Version:** {py_version['version']}",
        "",
        "---",
        "",
        "## Executive Summary",
        "",
    ]

    # Count issues
    package_issues = sum(1 for pkg, (avail, _) in packages.items() if not avail)
    service_issues = sum(1 for key in ['neo4j', 'redis', 'chromadb'] if not detection.get(key, False))
    config_issues = sum(1 for key in ['anthropic', 'openai'] if not detection.get(key, False))

    lines.extend([
        f"- **Package Issues:** {package_issues}",
        f"- **Service Issues:** {service_issues}",
        f"- **Configuration Issues:** {config_issues}",
        f"- **Python Warnings:** {len(py_version['warnings'])}",
        "",
        "---",
        "",
        "## Section 1: Python Package Issues",
        "",
        "| Package | Status | Error |",
        "|---------|--------|-------|",
    ])

    for pkg, (available, error) in packages.items():
        status = "OK" if available else "MISSING"
        error_msg = error[:50] + "..." if error and len(error) > 50 else (error or "-")
        lines.append(f"| {pkg} | {status} | {error_msg} |")

    if py_version['warnings']:
        lines.extend([
            "",
            "### Python Version Warnings",
            "",
        ])
        for warning in py_version['warnings']:
            lines.append(f"- {warning}")

    lines.extend([
        "",
        "---",
        "",
        "## Section 2: External Service Requirements",
        "",
        "| Service | Status | Required For |",
        "|---------|--------|--------------|",
    ])

    services = [
        ("Ollama", detection['ollama'], "Local LLM testing"),
        ("Docker", detection['docker'], "Sandboxed execution (Gap 4)"),
        ("Docker Sandbox", detection['docker_sandbox'], "Production executor"),
        ("Neo4j", detection.get('neo4j', False), "Knowledge graph"),
        ("Redis", detection.get('redis', False), "Distributed caching"),
        ("ChromaDB", detection.get('chromadb', False), "Vector embeddings"),
        ("SQLite", detection['database'], "Data persistence"),
    ]

    for name, status, purpose in services:
        status_str = "OK" if status else "NOT CONFIGURED"
        lines.append(f"| {name} | {status_str} | {purpose} |")

    lines.extend([
        "",
        "---",
        "",
        "## Section 3: Configuration Gaps",
        "",
        "| Variable | Status | Description |",
        "|----------|--------|-------------|",
    ])

    config_vars = [
        ("ANTHROPIC_API_KEY", detection['anthropic'], "Anthropic LLM provider"),
        ("OPENAI_API_KEY", detection['openai'], "OpenAI/Ollama provider"),
        ("NEO4J_URI", detection.get('neo4j', False), "Neo4j connection"),
        ("REDIS_URL", detection.get('redis', False), "Redis connection"),
        ("SEMANTIC_SCHOLAR_API_KEY", detection.get('semantic_scholar', False), "Literature search API"),
    ]

    for var, status, desc in config_vars:
        status_str = "SET" if status else "NOT SET"
        lines.append(f"| {var} | {status_str} | {desc} |")

    lines.extend([
        "",
        "---",
        "",
        "## Section 4: Service Availability Matrix",
        "",
        "| Test Category | Anthropic | Docker | Neo4j | Redis | ChromaDB |",
        "|---------------|-----------|--------|-------|-------|----------|",
    ])

    for category, requirements in service_matrix.items():
        row = f"| {category} |"
        for service in ['anthropic', 'docker', 'neo4j', 'redis', 'chromadb']:
            row += f" {requirements.get(service, 'N/A')} |"
        lines.append(row)

    lines.extend([
        "",
        "---",
        "",
        "## Section 5: Remediation Strategy",
        "",
        "### Tier 1: Quick Wins (< 1 day)",
        "",
    ])

    quick_wins = []
    if not detection['ollama']:
        quick_wins.append("- Install and start Ollama: `ollama serve`")
    if package_issues > 0:
        quick_wins.append("- Install missing packages: `pip install -e \".[dev]\"`")
    if not detection['anthropic'] and not detection['openai']:
        quick_wins.append("- Set API key: `export ANTHROPIC_API_KEY=sk-ant-...`")

    lines.extend(quick_wins if quick_wins else ["- No quick wins identified"])

    lines.extend([
        "",
        "### Tier 2: Medium Effort (1-3 days)",
        "",
    ])

    medium_effort = []
    if not detection['docker_sandbox']:
        medium_effort.append("- Build Docker sandbox: `./scripts/setup-docker.sh`")
    if not detection.get('neo4j', False):
        medium_effort.append("- Set up Neo4j: `docker run neo4j:latest`")

    lines.extend(medium_effort if medium_effort else ["- No medium effort items identified"])

    lines.extend([
        "",
        "### Tier 3: Significant Work (> 3 days)",
        "",
        "- Replace incompatible packages (e.g., arxiv on Python 3.11+)",
        "- Implement missing async features",
        "- Refactor for better test isolation",
        "",
        "---",
        "",
        "## Recommended Test Tier",
        "",
    ])

    from .provider_detector import recommend_test_tier, recommend_provider
    tier = recommend_test_tier(detection)
    provider = recommend_provider(detection)

    lines.extend([
        f"Based on current infrastructure: **{tier}**",
        f"",
        f"Recommended provider: **{provider}**",
        "",
        "---",
        "",
        "*Generated by Kosmos E2E Testing Skill*",
    ])

    report = "\n".join(lines)

    if output_path is None:
        output_path = Path(project_root) / "E2E_TESTING_DEPENDENCY_REPORT.md"

    Path(output_path).write_text(report)
    print(f"Dependency report saved to: {output_path}")

    return report


if __name__ == "__main__":
    # Demo with sample results
    sample_results = {
        "tier": "e2e",
        "provider": "local-reasoning",
        "passed": 42,
        "failed": 2,
        "skipped": 5,
        "total": 49,
        "duration": 245.5,
        "success": False,
        "coverage": 78.5,
    }

    print(generate_report(sample_results, format="text"))

    # Also generate dependency report
    print("\n" + "=" * 60 + "\n")
    generate_dependency_report()
